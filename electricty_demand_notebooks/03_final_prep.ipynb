{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "623ef974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Data Preparation for Electricity Demand ---\n",
      "1. Loading combined data...\n",
      "  - Columns renamed to snake_case. Initial shape: (434590, 22)\n",
      "  - 'settlement_period' and 'nd' coerced to numeric types.\n",
      "  - Datetime Index created and set.\n",
      "  - Extracted year, month, hour, day_of_week, and is_weekend.\n",
      "  - Applied time-series interpolation to 15 columns.\n",
      "  - Applied One-Hot Encoding to 'day_name'.\n",
      "\n",
      "Final number of columns: 30\n",
      "\n",
      "2. Saving final model-ready file to Parquet...\n",
      "\n",
      "--- Final Prep Complete! ---\n",
      "Model-ready data saved to: ../data/electricity/processed/demand_model_ready.parquet\n",
      "Final DataFrame shape: (434590, 30)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress minor warnings for a clean pipeline run\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# --- 1. FILE PATHS ---\n",
    "# Input file name confirmed in earlier steps\n",
    "input_parquet_file = '../data/electricity/processed/demanddata_combined.parquet' \n",
    "final_parquet_file_path = '../data/electricity/processed/demand_model_ready.parquet'\n",
    "\n",
    "print(\"--- Starting Final Data Preparation for Electricity Demand ---\")\n",
    "\n",
    "# --- 2. LOAD DATA ---\n",
    "print(\"1. Loading combined data...\")\n",
    "try:\n",
    "    df = pd.read_parquet(input_parquet_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: Input file not found at {input_parquet_file}. Please check path and run 01/02 notebooks.\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. CORE CLEANING & TYPE FIXES ---\n",
    "\n",
    "# A. Column Renaming (Snake Case)\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "print(f\"  - Columns renamed to snake_case. Initial shape: {df.shape}\")\n",
    "\n",
    "# B. Type Corrections (Crucial Fixes from EDA)\n",
    "# 1. Convert 'settlement_period' to integer for datetime calculation\n",
    "df['settlement_period'] = df['settlement_period'].astype(int)\n",
    "# 2. Convert the target column 'nd' to numeric (float), coercing any non-numeric strings to NaN.\n",
    "df['nd'] = pd.to_numeric(df['nd'], errors='coerce') \n",
    "print(\"  - 'settlement_period' and 'nd' coerced to numeric types.\")\n",
    "\n",
    "# --- 4. TIME-SERIES FEATURE ENGINEERING ---\n",
    "\n",
    "# C. Create Accurate Datetime Index\n",
    "df['settlement_date'] = pd.to_datetime(df['settlement_date'])\n",
    "df['time_offset'] = pd.to_timedelta((df['settlement_period'] - 1) * 30, unit='m')\n",
    "df['datetime'] = df['settlement_date'] + df['time_offset']\n",
    "\n",
    "df.set_index('datetime', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "df.drop(columns=['settlement_date', 'settlement_period', 'time_offset'], inplace=True)\n",
    "print(\"  - Datetime Index created and set.\")\n",
    "\n",
    "\n",
    "# D. Extract Temporal Features\n",
    "df['year'] = df.index.year\n",
    "df['month'] = df.index.month\n",
    "df['hour'] = df.index.hour\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['day_name'] = df.index.day_name()\n",
    "\n",
    "print(\"  - Extracted year, month, hour, day_of_week, and is_weekend.\")\n",
    "\n",
    "# --- 5. DATA QUALITY & ENCODING ---\n",
    "\n",
    "# E. Missing Value Imputation (Linear Interpolation)\n",
    "# This handles both initial NaNs and those created by pd.to_numeric(errors='coerce')\n",
    "cols_to_impute = df.columns[df.isnull().any()].tolist() # Get all columns with NaNs\n",
    "\n",
    "if cols_to_impute:\n",
    "    # Use time-series linear interpolation\n",
    "    df[cols_to_impute] = df[cols_to_impute].interpolate(method='time', axis=0)\n",
    "    # Fill any remaining NaNs at the start/end (very rare, but safe)\n",
    "    df[cols_to_impute] = df[cols_to_impute].ffill().bfill() \n",
    "    print(f\"  - Applied time-series interpolation to {len(cols_to_impute)} columns.\")\n",
    "else:\n",
    "    print(\"  - No missing values found for imputation.\")\n",
    "\n",
    "\n",
    "# F. Categorical Encoding (One-Hot Encoding)\n",
    "# One-hot encode the day_name feature\n",
    "df = pd.get_dummies(df, columns=['day_name'], drop_first=True, prefix='day')\n",
    "print(\"  - Applied One-Hot Encoding to 'day_name'.\")\n",
    "# Drop the simple day_of_week column as its information is now encoded\n",
    "df.drop(columns=['day_of_week'], inplace=True)\n",
    "\n",
    "print(f\"\\nFinal number of columns: {len(df.columns)}\")\n",
    "\n",
    "# --- 6. FINAL SAVE ---\n",
    "print(\"\\n2. Saving final model-ready file to Parquet...\")\n",
    "# Ensure the directory exists\n",
    "os.makedirs(os.path.dirname(final_parquet_file_path), exist_ok=True) \n",
    "\n",
    "# Save the final file with the index (datetime) intact\n",
    "df.to_parquet(final_parquet_file_path, index=True) \n",
    "\n",
    "print(\"\\n--- Final Prep Complete! ---\")\n",
    "print(f\"Model-ready data saved to: {final_parquet_file_path}\")\n",
    "print(f\"Final DataFrame shape: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
