{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbed6354",
   "metadata": {},
   "source": [
    "## 03 - Final Data Preparation Pipeline\n",
    "\n",
    "**Project:** UK Housing Price Paid Records\n",
    "\n",
    "**Purpose:** To serve as the final, clean, and efficient data pipeline. This notebook takes the raw CSV file and applies all the necessary cleaning, renaming, and feature engineering steps (discovered in 02\\_eda.ipynb) to create the ultimate model-ready Parquet file. **NO GRAPHS or analytical prints should be present.**\n",
    "\n",
    "**Team Member(s):** Tymo Verhaegen\n",
    "\n",
    "**Output File:** `../data/housing/processed/price_paid_model_ready.parquet`\n",
    "\n",
    "**Date Last Run:** 06/11/2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ea9adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Final Data Preparation ---\n",
      "1. Loading Parquet from: ../data/housing/processed/price_paid_init.parquet\n",
      "   -> Load successful. Raw records: 22,489,348\n",
      "\n",
      "2. Applying cleaning and feature engineering...\n",
      "   -> Columns renamed to snake_case.\n",
      "   -> Removed 92 low-value transactions (Price <= £1).\n",
      "   -> Duration updated to ordered category.\n",
      "\n",
      "Final number of columns: 10\n",
      "Final columns: ['price', 'sale_date', 'property_type', 'old_new', 'duration', 'town_city', 'district', 'county', 'record_status___monthly_file_only', 'sale_year']\n",
      "\n",
      "3. Saving final model-ready file to Parquet...\n",
      "\n",
      "--- Final Prep Complete! ---\n",
      "Model-ready data saved to: ../data/housing/processed/price_paid_model_ready.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Define Paths and Settings ---\n",
    "print(\"--- Starting Final Data Preparation ---\")\n",
    "input_file = \"../data/housing/processed/price_paid_init.parquet\" \n",
    "processed_dir = '../data/housing/processed/' \n",
    "final_parquet_file_path = os.path.join(processed_dir, 'price_paid_model_ready.parquet') \n",
    "\n",
    "# Ensure the target directory exists\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "# --- 2. LOAD THE PARQUET FILE ---\n",
    "# FIX: Removed dtype, parse_dates, low_memory, and engine. \n",
    "# Parquet reads schema automatically.\n",
    "print(f\"1. Loading Parquet from: {input_file}\")\n",
    "try:\n",
    "    df = pd.read_parquet(input_file)\n",
    "    print(f\"   -> Load successful. Raw records: {len(df):,}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"   -> ERROR: File not found. Path: {input_file}. Aborting.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"   -> ERROR reading Parquet file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. APPLY CLEANING AND FEATURE ENGINEERING ---\n",
    "print(\"\\n2. Applying cleaning and feature engineering...\")\n",
    "\n",
    "# A. Column Renaming\n",
    "# We do this FIRST so we can reference easy lowercase names later\n",
    "def clean_col_name(col):\n",
    "    return col.lower().replace(' ', '_').replace('/', '_').replace('-', '_').strip()\n",
    "\n",
    "df.columns = [clean_col_name(col) for col in df.columns]\n",
    "print(\"   -> Columns renamed to snake_case.\")\n",
    "\n",
    "# B. OPTIMIZE DATA TYPES (Memory Management)\n",
    "# We apply this AFTER renaming.\n",
    "# Using 'category' for strings saves massive amounts of RAM.\n",
    "cat_cols = [\n",
    "    'property_type', 'old_new', 'duration', 'town_city', \n",
    "    'district', 'county', 'ppdcategory_type', 'record_status_monthly_file_only'\n",
    "]\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "# C. Feature Engineering (Add Year and Fix Date)\n",
    "# Check if it's already a datetime object (Parquet preserves this)\n",
    "if not pd.api.types.is_datetime64_any_dtype(df['date_of_transfer']):\n",
    "    df['date_of_transfer'] = pd.to_datetime(df['date_of_transfer'])\n",
    "\n",
    "df['sale_year'] = df['date_of_transfer'].dt.year.astype('int16')\n",
    "df.rename(columns={'date_of_transfer': 'sale_date'}, inplace=True)\n",
    "\n",
    "# D. Handle Outliers\n",
    "count_low_price = len(df[df['price'] <= 1])\n",
    "df = df[df['price'] > 1].copy()\n",
    "print(f\"   -> Removed {count_low_price:,} low-value transactions (Price <= £1).\")\n",
    "\n",
    "# E. Drop Unique ID (if it exists)\n",
    "if 'transaction_unique_identifier' in df.columns:\n",
    "    df.drop(columns=['transaction_unique_identifier'], inplace=True)\n",
    "\n",
    "# F. Fix 'duration' (Leasehold vs Freehold)\n",
    "# Standardize the mapping\n",
    "duration_mapping = {'L': 'Leasehold', 'F': 'Freehold', 'U': 'Unknown'}\n",
    "# Only map if the values are actually L/F (Parquet might have saved them as full words already)\n",
    "unique_vals = df['duration'].unique()\n",
    "if 'L' in unique_vals or 'F' in unique_vals:\n",
    "    df['duration'] = df['duration'].map(duration_mapping)\n",
    "\n",
    "# Set order\n",
    "duration_order = ['Leasehold', 'Freehold']\n",
    "df['duration'] = pd.Categorical(df['duration'], categories=duration_order, ordered=True)\n",
    "print(\"   -> Duration updated to ordered category.\")\n",
    "\n",
    "# G. Drop redundant columns\n",
    "# Note: 'ppd_category_type' usually becomes 'ppdcategory_type' after cleaning spaces\n",
    "cols_to_drop = ['ppdcategory_type', 'record_status_monthly_file_only', 'record_status']\n",
    "df.drop(columns=cols_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"\\nFinal number of columns: {len(df.columns)}\")\n",
    "print(f\"Final columns: {list(df.columns)}\")\n",
    "\n",
    "# --- 4. FINAL SAVE ---\n",
    "print(\"\\n3. Saving final model-ready file to Parquet...\")\n",
    "df.to_parquet(final_parquet_file_path, index=False)\n",
    "\n",
    "print(\"\\n--- Final Prep Complete! ---\")\n",
    "print(f\"Model-ready data saved to: {final_parquet_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
