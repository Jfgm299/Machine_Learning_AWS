{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1472c817-84c1-40e9-9c3f-f2cd7a71157f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "AWS Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the current SageMaker session and region\n",
    "sagemaker_session = sagemaker.Session()\n",
    "aws_region = sagemaker_session.boto_region_name\n",
    "\n",
    "print(f\"AWS Region: {aws_region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4feec7ca-cfe6-4356-bb1e-851fac607b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bucket: sagemaker-project-us-east-1-1763116930\n",
      "Successfully created bucket: sagemaker-project-us-east-1-1763116930\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import time\n",
    "# 1. Choose a unique bucket name\n",
    "# S3 bucket names must be globally unique!\n",
    "\n",
    "\n",
    "unique_id = int(time.time())\n",
    "bucket_name = f\"sagemaker-project-{aws_region}-{unique_id}\"\n",
    "print(f\"Creating bucket: {bucket_name}\")\n",
    "\n",
    "# 2. Create an S3 client\n",
    "s3_client = boto3.client(\"s3\", region_name=aws_region)\n",
    "\n",
    "# 3. Define the bucket configuration\n",
    "# The 'LocationConstraint' is necessary for all regions *except* us-east-1\n",
    "if aws_region == \"us-east-1\":\n",
    "    s3_client.create_bucket(Bucket=bucket_name)\n",
    "else:\n",
    "    create_bucket_configuration = {\n",
    "        'LocationConstraint': aws_region\n",
    "    }\n",
    "    s3_client.create_bucket(\n",
    "        Bucket=bucket_name,\n",
    "        CreateBucketConfiguration=create_bucket_configuration\n",
    "    )\n",
    "\n",
    "print(f\"Successfully created bucket: {bucket_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9be9d7-f2b5-4f86-a804-b6f6b23f4ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset from local file: 'price_paid_model_ready.parquet'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "\n",
    "bucket_name = 'sagemaker-project-us-east-1-1763116930'\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Load the local file ---\n",
    "\n",
    "# vvv MAKE SURE THIS IS YOUR UPLOADED FILE'S NAME vvv\n",
    "local_file_path = \"price_paid_model_ready.parquet\" # <-- CHANGE TO YOUR UPLOADED FILE'S NAME\n",
    "# ^^^ MAKE SURE THIS IS YOUR UPLOADED FILE'S NAME ^^^\n",
    "\n",
    "try:\n",
    "    print(f\"Loading full dataset from local file: '{local_file_path}'\")\n",
    "    df_full = pd.read_parquet(local_file_path)\n",
    "    print(f\"Loaded {len(df_full)} total records.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{local_file_path}' was not found in your notebook environment.\")\n",
    "    print(\"This means the upload may have failed or the file name is misspelled.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Split the data by Year (Time-Series Split) ---\n",
    "#\n",
    "# Using your 'sale_year' column.\n",
    "\n",
    "# *** YOU MUST CHANGE THIS ***\n",
    "year_column = 'sale_year' # <-- This is your column name\n",
    "split_year = 2014         # <-- CHANGE TO YOUR DESIRED SPLIT YEAR\n",
    "                          # This means years BEFORE 2021 are for training,\n",
    "                          # and 2021 AND LATER are for validation.\n",
    "\n",
    "try:\n",
    "    print(f\"Splitting data on column '{year_column}' before year {split_year}...\")\n",
    "    \n",
    "    # Check if the column is numeric. If it's text, we try to convert it.\n",
    "    if not pd.api.types.is_numeric_dtype(df_full[year_column]):\n",
    "        print(f\"Warning: Column '{year_column}' is not numeric. Attempting to convert to integer...\")\n",
    "        df_full[year_column] = df_full[year_column].astype(int)\n",
    "\n",
    "    # Simple integer comparison for the split\n",
    "    df_train = df_full[df_full[year_column] < split_year]\n",
    "    df_validation = df_full[df_full[year_column] >= split_year]\n",
    "    \n",
    "    if len(df_train) == 0 or len(df_validation) == 0:\n",
    "        print(f\"Warning: Your split year '{split_year}' or column '{year_column}' resulted in an empty set.\")\n",
    "        print(f\"Training records: {len(df_train)}, Validation records: {len(df_validation)}\")\n",
    "    else:\n",
    "        print(f\"Split complete.\")\n",
    "        print(f\"Training records:   {len(df_train)} (Years: {df_train[year_column].min()} - {df_train[year_column].max()})\")\n",
    "        print(f\"Validation records: {len(df_validation)} (Years: {df_validation[year_column].min()} - {df_validation[year_column].max()})\")\n",
    "\n",
    "\n",
    "except KeyError:\n",
    "    print(f\"Error: The column '{year_column}' was not found in your Parquet file.\")\n",
    "    print(\"Please change the 'year_column' variable in this script to the correct column name.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the split: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Save the new split files locally ---\n",
    "local_train_file = \"train.parquet\"\n",
    "local_validation_file = \"validation.parquet\"\n",
    "\n",
    "df_train.to_parquet(local_train_file, index=False)\n",
    "df_validation.to_parquet(local_validation_file, index=False)\n",
    "print(\"Local train.parquet and validation.parquet files created.\")\n",
    "\n",
    "# --- 4. Upload the new files to separate S3 prefixes ---\n",
    "# (These variables should exist from the cell where you created the bucket)\n",
    "# s3_client, bucket_name, aws_region\n",
    "\n",
    "s3_prefix_train = \"data/train\"\n",
    "s3_prefix_validation = \"data/validation\"\n",
    "\n",
    "# These are the final paths you will give to the SageMaker training job\n",
    "s3_input_train_path = f\"s3://{bucket_name}/{s3_prefix_train}\"\n",
    "s3_input_validation_path = f\"s3://{bucket_name}/{s3_prefix_validation}\"\n",
    "\n",
    "# Upload the TRAINING file\n",
    "try:\n",
    "    print(f\"Uploading {local_train_file} to {s3_input_train_path}/\")\n",
    "    s3_client.upload_file(\n",
    "        Filename=local_train_file,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{s3_prefix_train}/train.parquet\" # Saves it as data/train/train.parquet\n",
    "    )\n",
    "\n",
    "    # Upload the VALIDATION file\n",
    "    print(f\"Uploading {local_validation_file} to {s3_input_validation_path}/\")\n",
    "    s3_client.upload_file(\n",
    "        Filename=local_validation_file,\n",
    "        Bucket=bucket_name,\n",
    "        Key=f\"{s3_prefix_validation}/validation.parquet\" # Saves it as data/validation/validation.parquet\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- All steps complete! ---\")\n",
    "    print(f\"Your training data is ready at: {s3_input_train_path}\")\n",
    "    print(f\"Your validation data is ready at: {s3_input_validation_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the S3 upload: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e427f286-1ae0-477a-883a-02e429cd065b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0.tar.gz (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from lightgbm) (1.15.2)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.6.0-py3-none-linux_x86_64.whl size=2737777 sha256=3d0e867e27fd7a44bb6a2ab89a9fb543cb44a452fb685073674f0f2de663eee2\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/bb/db/6d/7814aed03437129dc284a055c084f201b765deb54b6908efab\n",
      "Successfully built lightgbm\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n",
      "Using training data: s3://sagemaker-project-us-east-1-1763114558/data/train\n",
      "Using validation data: s3://sagemaker-project-us-east-1-1763114558/data/validation\n",
      "Using IAM Role: arn:aws:iam::550770041202:role/LabRole\n",
      "--- Starting LightGBM Training Job ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2025-11-14-13-57-11-343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-14 13:57:13 Starting - Starting the training job...\n",
      "2025-11-14 13:57:28 Starting - Preparing the instances for training...\n",
      "2025-11-14 13:57:51 Downloading - Downloading input data...\n",
      "2025-11-14 13:58:26 Downloading - Downloading the training image......\n",
      "2025-11-14 13:59:32 Training - Training image download completed. Training in progress..\u001b[34m/miniconda3/lib/python3.9/site-packages/sagemaker_containers/_server.py:22: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:36,665 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:36,671 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:36,674 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:36,693 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,002 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,031 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,036 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,061 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,064 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,082 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.large\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"learning_rate\": 0.05,\n",
      "        \"n_estimators\": 200,\n",
      "        \"num_leaves\": 31\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.large\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2025-11-14-13-57-11-343\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-550770041202/sagemaker-scikit-learn-2025-11-14-13-57-11-343/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"main.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"learning_rate\":0.05,\"n_estimators\":200,\"num_leaves\":31}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=main.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.large\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=main\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-550770041202/sagemaker-scikit-learn-2025-11-14-13-57-11-343/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.large\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"learning_rate\":0.05,\"n_estimators\":200,\"num_leaves\":31},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"sagemaker-scikit-learn-2025-11-14-13-57-11-343\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-550770041202/sagemaker-scikit-learn-2025-11-14-13-57-11-343/source/sourcedir.tar.gz\",\"module_name\":\"main\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"main.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--learning_rate\",\"0.05\",\"--n_estimators\",\"200\",\"--num_leaves\",\"31\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_N_ESTIMATORS=200\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LEAVES=31\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python39.zip:/miniconda3/lib/python3.9:/miniconda3/lib/python3.9/lib-dynload:/miniconda3/lib/python3.9/site-packages:/miniconda3/lib/python3.9/site-packages/setuptools/_vendor\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python main.py --learning_rate 0.05 --n_estimators 200 --num_leaves 31\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,084 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-11-14 13:59:37,085 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mWorking\u001b[0m\n",
      "\u001b[34mPackage 'lightgbm' not found. Installing...\u001b[0m\n",
      "\u001b[34mCollecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.0 in /miniconda3/lib/python3.9/site-packages (from lightgbm) (1.24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /miniconda3/lib/python3.9/site-packages (from lightgbm) (1.8.0)\u001b[0m\n",
      "\u001b[34mDownloading lightgbm-4.6.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 138.2 MB/s  0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: lightgbm\u001b[0m\n",
      "\u001b[34mSuccessfully installed lightgbm-4.6.0\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.2 -> 25.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPackage 'scikit-learn' not found. Installing...\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /miniconda3/lib/python3.9/site-packages (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.3 in /miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.3.2 in /miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /miniconda3/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.2 -> 25.3\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m--- Starting LightGBM Training ---\u001b[0m\n",
      "\u001b[34mArguments: Namespace(model_dir='/opt/ml/model', train='/opt/ml/input/data/train', validation='/opt/ml/input/data/validation', num_leaves=31, learning_rate=0.05, n_estimators=200)\u001b[0m\n",
      "\u001b[34mLoading data from /opt/ml/input/data/train/train.parquet\u001b[0m\n",
      "\u001b[34mLoading data from /opt/ml/input/data/validation/validation.parquet\u001b[0m\n",
      "\u001b[34mCategorical: ['property_type', 'old/new', 'duration', 'town/city', 'district', 'county', 'ppdcategory_type']\u001b[0m\n",
      "\u001b[34mNumeric: ['sale_year']\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.932189 seconds.\u001b[0m\n",
      "\u001b[34mYou can set `force_col_wise=true` to remove the overhead.\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Total Bins 1470\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Number of data points in the train set: 17612838, number of used features: 8\u001b[0m\n",
      "\u001b[34m[LightGBM] [Info] Start training from score 149066.623794\u001b[0m\n",
      "\u001b[34mTraining until validation scores don't improve for 50 rounds\u001b[0m\n",
      "\u001b[34mDid not meet early stopping. Best iteration is:\u001b[0m\n",
      "\u001b[34m[200]#011valid_0's l2: 5.49522e+11\u001b[0m\n",
      "\u001b[34mValidation R²: 0.0804\u001b[0m\n",
      "\u001b[34m--- Training Complete ---\u001b[0m\n",
      "\u001b[34m2025-11-14 14:05:16,858 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-11-14 14:05:42 Uploading - Uploading generated training model\n",
      "2025-11-14 14:05:42 Completed - Training job completed\n",
      "Training seconds: 470\n",
      "Billable seconds: 470\n",
      "--- Training Complete ---\n",
      "Model artifacts saved to: s3://sagemaker-us-east-1-550770041202/sagemaker-scikit-learn-2025-11-14-13-57-11-343/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "!pip install lightgbm\n",
    "# --- 1. Correct bucket ---\n",
    "bucket_name = \"sagemaker-project-us-east-1-1763114558\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# --- 2. Data paths ---\n",
    "s3_input_train_path = f\"s3://{bucket_name}/data/train\"\n",
    "s3_input_validation_path = f\"s3://{bucket_name}/data/validation\"\n",
    "\n",
    "print(f\"Using training data: {s3_input_train_path}\")\n",
    "print(f\"Using validation data: {s3_input_validation_path}\")\n",
    "print(f\"Using IAM Role: {role}\")\n",
    "\n",
    "# --- 3. Create the SageMaker Estimator ---\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"main.py\",\n",
    "    requirements_file=\"requirements.txt\",   # installs lightgbm\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 200,\n",
    "        \"num_leaves\": 31,\n",
    "        \"learning_rate\": 0.05\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 4. Define input channels ---\n",
    "data_channels = {\n",
    "    \"train\": s3_input_train_path,\n",
    "    \"validation\": s3_input_validation_path\n",
    "}\n",
    "\n",
    "# --- 5. Launch training job ---\n",
    "print(\"--- Starting LightGBM Training Job ---\")\n",
    "sklearn_estimator.fit(data_channels)\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "print(f\"Model artifacts saved to: {sklearn_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3519f9be-a1b7-4cd1-814a-11e895d9fa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking for files... ---\n",
      "\n",
      "Checking for files in: s3://sagemaker-project-us-east-1-1763114558/data/train/\n",
      "--- Found training files: ---\n",
      " - data/train/train.parquet\n",
      "\n",
      "Checking for files in: s3://sagemaker-project-us-east-1-1763114558/data/validation/\n",
      "--- Found validation files: ---\n",
      " - data/validation/validation.parquet\n",
      "\n",
      "--- Check Complete ---\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# --- 1. CONFIGURE YOUR BUCKET NAME ---\n",
    "# This MUST match the bucket you created\n",
    "bucket_name = \"sagemaker-project-us-east-1-1763114558\"\n",
    "# ---\n",
    "\n",
    "# Define the prefixes we are checking\n",
    "s3_prefix_train = \"data/train\"\n",
    "s3_prefix_validation = \"data/validation\"\n",
    "\n",
    "# Get the S3 client\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "print(f\"--- Checking for files... ---\")\n",
    "\n",
    "# --- 2. Check the TRAINING prefix ---\n",
    "print(f\"\\nChecking for files in: s3://{bucket_name}/{s3_prefix_train}/\")\n",
    "try:\n",
    "    response_train = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=s3_prefix_train\n",
    "    )\n",
    "    \n",
    "    # S3 list_objects_v2 returns the prefix itself as an object if it's empty\n",
    "    # so we check for objects *other* than just the folder.\n",
    "    files_found = [obj['Key'] for obj in response_train.get('Contents', []) if not obj['Key'].endswith('/')]\n",
    "    \n",
    "    if files_found:\n",
    "        print(\"--- Found training files: ---\")\n",
    "        for file_key in files_found:\n",
    "            print(f\" - {file_key}\")\n",
    "    else:\n",
    "        print(\"!!! NO TRAINING FILES FOUND in this prefix.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred listing training files: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Check the VALIDATION prefix ---\n",
    "print(f\"\\nChecking for files in: s3://{bucket_name}/{s3_prefix_validation}/\")\n",
    "try:\n",
    "    response_val = s3_client.list_objects_v2(\n",
    "        Bucket=bucket_name,\n",
    "        Prefix=s3_prefix_validation\n",
    "    )\n",
    "    \n",
    "    files_found = [obj['Key'] for obj in response_val.get('Contents', []) if not obj['Key'].endswith('/')]\n",
    "    \n",
    "    if files_found:\n",
    "        print(\"--- Found validation files: ---\")\n",
    "        for file_key in files_found:\n",
    "            print(f\" - {file_key}\")\n",
    "    else:\n",
    "        print(\"!!! NO VALIDATION FILES FOUND in this prefix.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred listing validation files: {e}\")\n",
    "\n",
    "print(\"\\n--- Check Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6078c8-8d01-4888-9c36-482b4f18ef97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
